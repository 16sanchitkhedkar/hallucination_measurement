# hallucination_measurement
Checking Hallucinations by Large Language Models

The aim of this project is to check hallucinations in the output generated by Large Language Models in regards to educational course content and minimize the same by fine tuning the model to minimize inaccuracies in the output